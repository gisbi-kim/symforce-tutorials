{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "03f90f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symforce uses symengine as backend\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import copy \n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "import open3d as o3d\n",
    "\n",
    "import symforce \n",
    "# symforce.set_log_level(\"warning\")\n",
    "symforce.set_log_level(\"ERROR\")\n",
    "print(f\"symforce uses {symforce.get_symbolic_api()} as backend\")\n",
    "\n",
    "from symforce.notebook_util import display\n",
    "import symforce.symbolic as sf\n",
    "from symforce.values import Values\n",
    "from symforce import ops\n",
    "from symforce.ops import StorageOps, GroupOps, LieGroupOps\n",
    "\n",
    "import symforce.opt.noise_models as nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a6ec069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "  \n",
    "# ref: https://www.geeksforgeeks.org/timing-functions-with-decorators-python/\n",
    "disp_timecost = True \n",
    "def timer(func):\n",
    "    # This function shows the execution time of \n",
    "    # the function object passed\n",
    "    def wrap_func(*args, **kwargs):\n",
    "        t1 = time()\n",
    "        result = func(*args, **kwargs)\n",
    "        t2 = time()\n",
    "        \n",
    "        if disp_timecost:\n",
    "            print(f'Function {func.__name__} executed in {(t2-t1):.4f}s')\n",
    "\n",
    "        return result\n",
    "    return wrap_func\n",
    "\n",
    "def np2o3d(nx3mat):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(nx3mat)\n",
    "    return pcd\n",
    "\n",
    "def to_o3dlineset(points, corres_idxes):\n",
    "    if len(points) == 0:\n",
    "        return None\n",
    "    \n",
    "    return o3d.geometry.LineSet(\n",
    "        points=o3d.utility.Vector3dVector(np.array(points)),\n",
    "        lines=o3d.utility.Vector2iVector(np.array(corres_idxes)),\n",
    "    )\n",
    "\n",
    "def probabilistic_false_corres_idx(init_idx, num_max, outlier_ratio=0.5):\n",
    "    if np.random.rand(1) > outlier_ratio:\n",
    "        # return ture_correspondenceness, corres_idx\n",
    "        return init_idx, True\n",
    "    else:\n",
    "        return int(np.random.randint(num_max, size=(1)).squeeze()), False\n",
    "#         return 1, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7f528c55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_val type is <class 'symforce.geo.matrix.Matrix31'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model parameters (as symbolic)\n",
    "scale    = sf.V1.symbolic(\"s\")\n",
    "transvec = sf.V3.symbolic(\"t\")\n",
    "rotvec   = sf.V3.symbolic(\"Theta\") # i.e., angle-axis parametrization\n",
    "rotmat   = LieGroupOps.from_tangent(sf.Rot3, rotvec) # for debug, display(rotmat.to_rotation_matrix())\n",
    "\n",
    "# Redisual (loss function)\n",
    "#  note: the rotation 'matrix' is used to formulate the below constraint, \n",
    "#        but it was parametrized as a 3-dim vector 'rotvec'!\n",
    "p_src        = sf.V3.symbolic(\"p_src\")     # p means a single 3D point \n",
    "p_tgt        = sf.V3.symbolic(\"p_tgt\") \n",
    "\n",
    "# p_tgt_est    = (rotmat * p_src)*scale + transvec # bug. sometimes converges but sometimes weired behaviors \n",
    "p_tgt_est    = ((rotmat * p_src) + transvec)*scale # The constraint: see the eq149 of https://ethaneade.com/lie.pdf\n",
    "    # for the Sim(3) details, see\n",
    "        # Scale Drift-Aware Large Scale Monocular SLAM (RSS 2020)\n",
    "    # and see the eq149 of https://ethaneade.com/lie.pdf\n",
    "\n",
    "error_val = p_tgt - p_tgt_est\n",
    "print(f\"error_val type is {type(error_val)}\")\n",
    "\n",
    "def robust_loss(error_V3: sf.V3):\n",
    "    \"\"\"\n",
    "    see the class BarronNoiseModel(ScalarNoiseModel) definition in noise_models.py\n",
    "        alpha: Controls shape and convexity of the loss function. Notable values:\n",
    "            alpha = 2 -> L2 loss\n",
    "            alpha = 1 -> Pseudo-huber loss\n",
    "            alpha = 0 -> Cauchy loss\n",
    "            alpha = -2 -> Geman-McClure loss\n",
    "            alpha = -inf -> Welsch loss\n",
    "        delta: Determines the transition point from quadratic to robust. Similar to \"delta\" as used\n",
    "            by the pseudo-huber loss function.\n",
    "        scalar_information: Scalar representing the inverse of the variance of an element of the\n",
    "            unwhitened residual. Conceptually, we use \"scalar_information\" to whiten (in a\n",
    "            probabalistic sense) the unwhitened residual before passing it through the Barron loss.\n",
    "        x_epsilon: Small value used for handling the singularity at x == 0.\n",
    "        alpha_epsilon: Small value used for handling singularities around alpha.\n",
    "    \"\"\"\n",
    "\n",
    "    alpha = 0\n",
    "    delta = 0.2\n",
    "    scalar_information = 5.0\n",
    "    epsilon = 1.0e-6\n",
    "\n",
    "    noise_model = nm.BarronNoiseModel(\n",
    "        alpha=alpha, delta=delta, scalar_information=scalar_information, x_epsilon=epsilon\n",
    "    )\n",
    "\n",
    "    robustified_error = sf.V1(noise_model.error(error_V3)) # robust loss \n",
    "\n",
    "#     robustified_error = error_V3.compute_AtA() #non robust loss \n",
    "    \n",
    "    return robustified_error\n",
    "\n",
    "error_model = robust_loss(error_val) \n",
    "\n",
    "# residual jacobian\n",
    "#  this is the powerful moment of symforce. It automatically generate the Jacobian equations explicitly. \n",
    "Je_trans_model = error_model.jacobian(transvec)\n",
    "Je_rot_model = error_model.jacobian(rotvec)\n",
    "Je_scale_model = error_model.jacobian(scale)\n",
    "\n",
    "# residual debug \n",
    "is_vis_jacobians = False\n",
    "\n",
    "def disp_info(elm, name=''):\n",
    "    print(\"=========INFO==========\")\n",
    "    print(f\"The shape and equation of {name}:\")\n",
    "    display(elm.shape)\n",
    "    display(elm)\n",
    "    print(\"=======================\\n\")\n",
    "\n",
    "if is_vis_jacobians:\n",
    "    disp_info(error_model, 'error_model')\n",
    "    disp_info(Je_rot_model, 'Je_rot')\n",
    "    disp_info(Je_trans_model, 'Je_trans')\n",
    "    disp_info(Je_scale_model, 'scale')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ad576453",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Sim(3) optimization state dimension \n",
    "ndim_state = 7\n",
    "ndim_loss = 1\n",
    "\n",
    "# The nonlinear icp alg. \n",
    "def evaluate_error_and_jacobian(src_pt: np.ndarray, tag_pt: np.ndarray, tf):\n",
    "    # note: transformation is 6dim vector on the tangent space (i.e., [rotvec, trans])  == lie algebra, aka se(3) (note that \"small\" se)\n",
    "    se3 = tf[:6] # [rotvec3dim, trans3dim]\n",
    "    s = tf[-1] # scale \n",
    "    \n",
    "    def inject_values(model):\n",
    "        model_evaluated = \\\n",
    "            model.subs({rotvec: sf.V3(se3[:3]), \\\n",
    "                        transvec: sf.V3(se3[3:]), \\\n",
    "                        scale: sf.V1(s), \\\n",
    "                        p_src: sf.V3(src_pt), \\\n",
    "                        p_tgt: sf.V3(tag_pt)})\n",
    "        return model_evaluated.to_numpy()\n",
    "        \n",
    "    error, Je_rot, Je_trans, Je_scale = \\\n",
    "        [inject_values(x) for x in [error_model, Je_rot_model, Je_trans_model, Je_scale_model]]\n",
    "\n",
    "    return error, Je_rot, Je_trans, Je_scale\n",
    "    \n",
    "@timer\n",
    "def icp_once(src, tgt, tf_init, skip=100, outlier_ratio=0.5, num_iters=30, fix_scale=False, verbose=False):\n",
    "\n",
    "    for _iter in range(num_iters):\n",
    "        num_pts = src.shape[0]\n",
    "\n",
    "        correct_corres_points = []\n",
    "        correct_corres_indexes = []\n",
    "        false_corres_points = []\n",
    "        false_corres_indexes = []\n",
    "\n",
    "        H = np.zeros((ndim_state, ndim_state))\n",
    "        b = np.zeros((ndim_state, 1))\n",
    "\n",
    "        # 1. gathering measurements\n",
    "        #  these should be parallelized with only locking the H++ and b++ block. C++ would be a choice for this job.\n",
    "        for pt_idx in range(num_pts):\n",
    "\n",
    "            # if \"true\" correspondence is given (this is an tutorial for education purpose), \n",
    "            # using a few points okay ..\n",
    "            if pt_idx % (skip+_iter) != 0:\n",
    "                continue # to save time cost, ealry return\n",
    "\n",
    "            # Here, we directly use the true-known pair (because this is a tutorial for educational purpose :)\n",
    "            #  In practice, (src_pt, tgt_pt) should be a correspondence (e.g., found by FPFH local featuer, see https://pcl.readthedocs.io/projects/tutorials/en/latest/fpfh_estimation.html)\n",
    "            src_corres_idx = pt_idx \n",
    "            tgt_corres_idx, is_true_corres \\\n",
    "                = probabilistic_false_corres_idx(pt_idx, num_pts-1, outlier_ratio=outlier_ratio) # true \n",
    "\n",
    "            src_pt, tgt_pt = src[src_corres_idx, :], tgt[tgt_corres_idx, :]\n",
    "            \n",
    "            e, Je_rot, Je_trans, Je_scale \\\n",
    "                = evaluate_error_and_jacobian(src_pt, tgt_pt, tf_init)\n",
    "                #   ps. To understand the details of this nonlinear iterative update steps, see http://www.diag.uniroma1.it//~labrococo/tutorial_icra_2016/icra16_slam_tutorial_grisetti.pdf \n",
    "                #       however, in the above slide's example, the jacobian was generated by hand as well as Euler angle space was used, not angle-axis.\n",
    "\n",
    "            if fix_scale:\n",
    "                e[-1] = 0.000001\n",
    "                Je_scale = np.array([[1]])\n",
    "\n",
    "            J = np.hstack((Je_rot, Je_trans, Je_scale)) \n",
    "                # this is 1x7 \n",
    "                #   1 is observation error model's output dimension \n",
    "                #      (in this tutorial, the error model is a norm of 3-dim error-state vector)\n",
    "                #   7 is the state dimension\n",
    "\n",
    "            sqrtW = np.array([[1., 1., 1., 1., 1., 1., 1.]])\n",
    "            J = sqrtW * J # whitened J. (element-wise multiplication)\n",
    "                            # i.e., J.t@W@J == (sqrtW@J).t @ (sqrtW@J) == ||sqrtW@J||2\n",
    "                          # In this case, make lower the sensitivity of the scale term will be benefit for the convergnece. \n",
    "                          #  This is an engineering. you should apply your physical prior or domain knowledge, or even empirically find it.\n",
    "            e[:3] *= 0.5 # this is also empirically important. \n",
    "            e[-1] *= 0.15 # this is also empirically important. \n",
    "                          # because translation changes along a few meters, but rotation and scales are lives in [0, 1]\n",
    "            \n",
    "            H = H + (J.T @ J) # H: 7x1 * 1x7 => thus H is 7x7\n",
    "            b = b + (J.T @ e) # b: 7x1 * 1x1 => thus b is 7x1\n",
    "                # to understatand the update eq, see https://darkpgmr.tistory.com/142\n",
    "\n",
    "            if verbose:\n",
    "                print(\"\\n=================\")\n",
    "                print(f\"{pt_idx} error is\\n{e.T}\")\n",
    "                print(f\"{pt_idx} Je_rot is\\n{Je_rot}\")\n",
    "                print(f\"{pt_idx} Je_trans is\\n{Je_trans}\")\n",
    "                print(f\"{pt_idx} J is\\n{J}\")\n",
    "                print(f\"{pt_idx} H is\\n{H}\")\n",
    "                print(f\"{pt_idx} b is\\n{b}\")\n",
    "\n",
    "            # debug \n",
    "            if is_true_corres:\n",
    "                correct_corres_points.append(src_pt)\n",
    "                correct_corres_points.append(tgt_pt)\n",
    "                correct_corres_indexes.append([len(correct_corres_indexes)*2, len(correct_corres_indexes)*2+1])\n",
    "            else:\n",
    "                false_corres_points.append(src_pt)\n",
    "                false_corres_points.append(tgt_pt)\n",
    "                false_corres_indexes.append([len(false_corres_indexes)*2, len(false_corres_indexes)*2+1])\n",
    "\n",
    "\n",
    "        # 2. update once \n",
    "        dtf = -np.linalg.solve(H, b).squeeze() # note the step direction is minus\n",
    "\n",
    "        \n",
    "        # debug\n",
    "        correct_corres_line_set = to_o3dlineset(correct_corres_points, correct_corres_indexes)\n",
    "        false_corres_line_set = to_o3dlineset(false_corres_points, false_corres_indexes)\n",
    "        line_sets = {\"correct\": correct_corres_line_set, \n",
    "                     \"false\": false_corres_line_set}\n",
    "\n",
    "        # update \n",
    "        strange_update_alram_thres = 100.0\n",
    "        if np.linalg.norm(dtf) > strange_update_alram_thres:\n",
    "            # strange_update_alram_thres is just arbitrarily selected because this is a toy problem \n",
    "            # if a weired update is detected, do not apply it.  \n",
    "            print(f\"dtf norm: {np.linalg.norm(dtf):.3f} is weired. Thus reject to update.\")\n",
    "            break\n",
    "\n",
    "        tf_init = tf_init + dtf # updated within the tangent space\n",
    "        print(f\"the estimated relative tf for iter {_iter} is {tf_init}\")\n",
    "\n",
    "    # final result \n",
    "    tf = tf_init\n",
    "\n",
    "    return tf, line_sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "07d5d96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PointCloud with 5205 points.\n",
      " The datset metric scale min [-1.0758  0.5284 -0.4984]\n",
      " The datset metric scale max [0.9524 1.9634 0.4083]\n",
      "\n",
      "true_rot_diff is\n",
      " [[-0.4226 -0.9063  0.    ]\n",
      " [ 0.9063 -0.4226  0.    ]\n",
      " [ 0.      0.      1.    ]]\n",
      "true_rot_diff_vec is\n",
      " [0.     0.     2.0071]\n",
      "true_trans_diff is\n",
      " [-0.1335  0.15    0.05  ]\n",
      "true_scale_diff is\n",
      " 5.0\n"
     ]
    }
   ],
   "source": [
    "# Data generatation  \n",
    "#  source \n",
    "dataset_name = \"dragon\"\n",
    "pcd0 = o3d.io.read_point_cloud(f'data/{dataset_name}.pcd')\n",
    "\n",
    "scale_up = 10\n",
    "pcd0_points_scaled_up = np.array(pcd0.points) * scale_up\n",
    "pcd0 = np2o3d(pcd0_points_scaled_up)\n",
    "print(pcd0)\n",
    "print(f\" The datset metric scale min {np.min(np.array(pcd0.points), 0)}\")\n",
    "print(f\" The datset metric scale max {np.max(np.array(pcd0.points), 0)}\")\n",
    "\n",
    "#  generate target \n",
    "def rpy2mat(rpy, deg=True):\n",
    "    return R.from_euler('xyz', rpy, degrees=deg).as_matrix()\n",
    "\n",
    "def rpy2vec(rpy, deg=True):\n",
    "    return R.from_euler('xyz', rpy, degrees=deg).as_rotvec()\n",
    "\n",
    "true_rot_diff_rpy = np.array([0, 0, 115]) # deg \n",
    "true_rot_diff = rpy2mat(true_rot_diff_rpy)\n",
    "true_rot_diff_vec = rpy2vec(true_rot_diff_rpy)\n",
    "true_trans_diff = np.array([-0.1335, 0.15, 0.05]) * (0.1*scale_up)\n",
    "true_scale_diff = 5.0\n",
    "\n",
    "print(f\"\\ntrue_rot_diff is\\n {true_rot_diff}\")\n",
    "print(f\"true_rot_diff_vec is\\n {true_rot_diff_vec}\")\n",
    "print(f\"true_trans_diff is\\n {true_trans_diff}\")\n",
    "print(f\"true_scale_diff is\\n {true_scale_diff}\")\n",
    "\n",
    "pcd1 = o3d.geometry.PointCloud()\n",
    "pcd1_Sim3_applied = true_scale_diff*(true_rot_diff @ np.array(pcd0.points).transpose()) + np.expand_dims(true_trans_diff, axis=-1)\n",
    "pcd1.points = o3d.utility.Vector3dVector(pcd1_Sim3_applied.transpose())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496d33a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_guess is [0.0243 0.0285 2.0356 0.8222 0.4198 0.7619 3.5001]\n",
      "\n",
      "======================================\n",
      "  ==========   iter 0   ==========\n",
      "======================================\n",
      "the estimated relative tf for iter 0 is [-0.1013  0.0273  2.0071  0.7276  0.3442  0.2578  3.2702]\n",
      "the estimated relative tf for iter 1 is [-0.0766 -0.0472  1.9578  0.4733  0.2325  0.2494  3.0476]\n",
      "the estimated relative tf for iter 2 is [-0.0827 -0.0884  2.015   0.2863  0.2181  0.227   3.0278]\n",
      "the estimated relative tf for iter 3 is [-0.1134 -0.118   2.0236  0.1062  0.1933  0.2217  3.0253]\n",
      "the estimated relative tf for iter 4 is [-0.1037 -0.084   2.011   0.0479  0.1436  0.1579  3.0873]\n",
      "the estimated relative tf for iter 5 is [-0.0643 -0.064   2.0104  0.0061  0.1103  0.1019  3.228 ]\n",
      "the estimated relative tf for iter 6 is [-0.0698 -0.0726  2.0462 -0.0945  0.1265  0.0983  3.3513]\n",
      "the estimated relative tf for iter 7 is [-0.0544 -0.0564  2.0446 -0.1987  0.1112  0.073   3.3385]\n",
      "the estimated relative tf for iter 8 is [-0.0655 -0.0411  2.0516 -0.2386  0.1043  0.0591  3.3751]\n",
      "the estimated relative tf for iter 9 is [-0.0574 -0.035   2.0474 -0.2181  0.0847  0.0437  3.5579]\n",
      "the estimated relative tf for iter 10 is [-0.0471 -0.0222  2.0474 -0.2059  0.0843  0.0276  3.7037]\n",
      "the estimated relative tf for iter 11 is [-0.0418 -0.0122  2.0345 -0.1869  0.0585  0.0214  3.8073]\n",
      "the estimated relative tf for iter 12 is [-0.0403 -0.0148  2.0441 -0.1826  0.0586  0.0223  3.9003]\n",
      "the estimated relative tf for iter 13 is [-0.0199 -0.007   2.0453 -0.1853  0.0635  0.0047  3.9961]\n",
      "the estimated relative tf for iter 14 is [-0.0279  0.007   2.0441 -0.1812  0.0607 -0.0026  4.0679]\n",
      "the estimated relative tf for iter 15 is [-0.0217  0.0116  2.0442 -0.1775  0.0634 -0.01    4.1374]\n",
      "the estimated relative tf for iter 16 is [-0.0083  0.0039  2.0443 -0.1734  0.0502 -0.0134  4.1588]\n",
      "the estimated relative tf for iter 17 is [-0.0055 -0.0026  2.0374 -0.1547  0.0405 -0.0067  4.246 ]\n",
      "the estimated relative tf for iter 18 is [ 0.0224  0.0083  2.0225 -0.1422  0.0248 -0.0213  4.3137]\n",
      "the estimated relative tf for iter 19 is [ 0.0019  0.0095  2.0137 -0.1353  0.0206 -0.015   4.3483]\n",
      "Function icp_once executed in 7.2078s\n",
      "\n",
      "==========estimation==========\n",
      "delta rotation:\n",
      "[[-0.4286 -0.9035  0.0056]\n",
      " [ 0.9035 -0.4286  0.0059]\n",
      " [-0.0029  0.0076  1.    ]]\n",
      "delta translation:\n",
      "[-0.1353  0.0206 -0.015 ]\n",
      "delta scale: 4.348\n",
      "\n",
      "======================================\n",
      "  ==========   iter 1   ==========\n",
      "======================================\n",
      "the estimated relative tf for iter 0 is [ 0.0092 -0.0018  0.0156 -0.161   0.0265  0.0262  1.0004]\n",
      "the estimated relative tf for iter 1 is [ 0.0022 -0.0039  0.0233 -0.2727  0.054   0.013   1.0044]\n",
      "the estimated relative tf for iter 2 is [-0.0057 -0.0061  0.0237 -0.3091  0.0487  0.0044  1.0118]\n",
      "the estimated relative tf for iter 3 is [-0.0065 -0.007   0.025  -0.3893  0.0356 -0.0079  1.0078]\n",
      "the estimated relative tf for iter 4 is [ 0.0132  0.0007  0.0244 -0.4138  0.003   0.014   1.0095]\n",
      "the estimated relative tf for iter 5 is [ 0.0114  0.0057  0.0161 -0.3737  0.0011 -0.0012  1.021 ]\n",
      "the estimated relative tf for iter 6 is [ 0.003   0.0185  0.008  -0.3659 -0.0252 -0.0828  1.0273]\n",
      "the estimated relative tf for iter 7 is [ 0.0015  0.0251  0.0022 -0.326  -0.0231 -0.1123  1.0385]\n",
      "the estimated relative tf for iter 8 is [ 0.0006  0.0308 -0.0006 -0.3185 -0.0319 -0.14    1.0442]\n",
      "the estimated relative tf for iter 9 is [ 0.0111  0.0037 -0.0041 -0.3559 -0.059  -0.002   1.0402]\n",
      "the estimated relative tf for iter 10 is [-0.0068 -0.0111  0.0032 -0.3526 -0.0194  0.037   1.0468]\n",
      "the estimated relative tf for iter 11 is [-0.0039  0.0169  0.0027 -0.3432 -0.0142 -0.0618  1.0538]\n",
      "the estimated relative tf for iter 12 is [ 0.0067  0.0381  0.0024 -0.3036 -0.0127 -0.1364  1.0596]\n",
      "the estimated relative tf for iter 13 is [ 0.0007  0.0115 -0.0043 -0.3356 -0.05   -0.0512  1.0537]\n",
      "the estimated relative tf for iter 14 is [ 0.0096  0.0028 -0.0087 -0.3106 -0.0666  0.0091  1.0552]\n",
      "the estimated relative tf for iter 15 is [-0.0027  0.002  -0.0056 -0.3092 -0.0486 -0.0234  1.0579]\n",
      "the estimated relative tf for iter 16 is [-0.0188  0.0131  0.0094 -0.3031  0.0226 -0.1007  1.0662]\n",
      "the estimated relative tf for iter 17 is [-0.0113 -0.0021  0.0126 -0.2956  0.0417 -0.0502  1.0665]\n",
      "the estimated relative tf for iter 18 is [ 0.0086 -0.0116  0.001  -0.3085 -0.0388  0.0234  1.062 ]\n",
      "the estimated relative tf for iter 19 is [ 0.0074 -0.0123 -0.0057 -0.3077 -0.0669  0.0438  1.0617]\n",
      "Function icp_once executed in 7.1234s\n",
      "\n",
      "==========estimation==========\n",
      "delta rotation:\n",
      "[[ 0.9999  0.0057 -0.0123]\n",
      " [-0.0058  1.     -0.0074]\n",
      " [ 0.0123  0.0075  0.9999]]\n",
      "delta translation:\n",
      "[-0.3077 -0.0669  0.0438]\n",
      "delta scale: 1.062\n",
      "\n",
      "======================================\n",
      "  ==========   iter 2   ==========\n",
      "======================================\n",
      "the estimated relative tf for iter 0 is [ 0.0023  0.0197 -0.0028  0.0515 -0.042  -0.0768  1.0011]\n",
      "the estimated relative tf for iter 1 is [ 0.0068  0.0162 -0.0017  0.0415 -0.0407 -0.042   1.0014]\n",
      "the estimated relative tf for iter 2 is [ 0.0131  0.0041 -0.0101  0.0388 -0.0833  0.0321  1.0008]\n",
      "the estimated relative tf for iter 3 is [-0.004  -0.0099 -0.0136  0.0459 -0.086   0.0445  1.0014]\n",
      "the estimated relative tf for iter 4 is [ 0.0082  0.0142 -0.0108  0.0202 -0.0946 -0.0276  0.9994]\n",
      "the estimated relative tf for iter 5 is [-0.0144 -0.0088 -0.0126 -0.0039 -0.105   0.0191  0.9947]\n",
      "the estimated relative tf for iter 6 is [ 0.0058 -0.0097 -0.0171 -0.0085 -0.1467  0.0763  0.9931]\n",
      "the estimated relative tf for iter 7 is [ 0.0057  0.0012 -0.0093 -0.0142 -0.0954  0.0376  0.9967]\n",
      "the estimated relative tf for iter 8 is [ 0.0036  0.0173 -0.0034 -0.0299 -0.0592 -0.0193  0.9999]\n",
      "the estimated relative tf for iter 9 is [ 0.0061  0.0015 -0.0048 -0.0139 -0.0561  0.0563  1.0016]\n",
      "the estimated relative tf for iter 10 is [-0.0094  0.0098 -0.0055 -0.0158 -0.0506 -0.0428  1.0024]\n",
      "the estimated relative tf for iter 11 is [ 0.0085  0.0146 -0.0042 -0.0132 -0.0603 -0.0325  1.0021]\n",
      "the estimated relative tf for iter 12 is [ 0.005   0.0191 -0.0066  0.0123 -0.0606 -0.0534  1.0066]\n",
      "the estimated relative tf for iter 13 is [ 0.0038  0.0214 -0.0009  0.0195 -0.0162 -0.0624  1.0102]\n",
      "the estimated relative tf for iter 14 is [ 0.0009  0.0142 -0.0121  0.0393 -0.0642 -0.0391  1.0115]\n",
      "the estimated relative tf for iter 15 is [ 0.0102  0.0371 -0.0145  0.078  -0.0585 -0.143   1.017 ]\n",
      "the estimated relative tf for iter 16 is [ 0.0087  0.0261 -0.0169  0.085  -0.0706 -0.0995  1.0164]\n",
      "the estimated relative tf for iter 17 is [ 0.0061  0.0096 -0.0124  0.0529 -0.056  -0.0153  1.0131]\n",
      "the estimated relative tf for iter 18 is [-0.0005 -0.0149  0.004  -0.0011  0.0094  0.0747  1.0125]\n",
      "the estimated relative tf for iter 19 is [ 0.0141 -0.0249 -0.0155  0.0423 -0.1042  0.1642  1.0142]\n",
      "Function icp_once executed in 6.5867s\n",
      "\n",
      "==========estimation==========\n",
      "delta rotation:\n",
      "[[ 0.9996  0.0153 -0.025 ]\n",
      " [-0.0157  0.9998 -0.0139]\n",
      " [ 0.0248  0.0143  0.9996]]\n",
      "delta translation:\n",
      "[ 0.0423 -0.1042  0.1642]\n",
      "delta scale: 1.014\n",
      "\n",
      "======================================\n",
      "  ==========   iter 3   ==========\n",
      "======================================\n",
      "the estimated relative tf for iter 0 is [-0.0153  0.027   0.0109 -0.0326 -0.0191 -0.1651  0.9878]\n",
      "the estimated relative tf for iter 1 is [-0.0189  0.0365  0.0066 -0.0193 -0.0236 -0.2167  0.9888]\n",
      "the estimated relative tf for iter 2 is [-0.0102  0.0546  0.0061 -0.0157 -0.0119 -0.2625  0.9903]\n",
      "the estimated relative tf for iter 3 is [-0.0252  0.0442  0.0101 -0.0608  0.0224 -0.2395  0.9899]\n",
      "the estimated relative tf for iter 4 is [-0.0123  0.0536  0.0106 -0.0419  0.0472 -0.2498  0.9932]\n",
      "the estimated relative tf for iter 5 is [-0.0219  0.0302  0.009  -0.0681  0.0217 -0.1736  0.9888]\n",
      "the estimated relative tf for iter 6 is [-0.0134  0.046   0.0028 -0.0403 -0.0007 -0.2152  0.9908]\n",
      "the estimated relative tf for iter 7 is [-0.0184  0.0503  0.0132 -0.0685  0.0638 -0.2573  0.9903]\n",
      "the estimated relative tf for iter 8 is [-0.0236  0.0524  0.0096 -0.0375  0.0679 -0.2832  0.9943]\n",
      "the estimated relative tf for iter 9 is [-0.0196  0.0565  0.0076 -0.0147  0.0668 -0.292   0.9981]\n",
      "the estimated relative tf for iter 10 is [-0.0213  0.0273 -0.0134  0.0062 -0.0438 -0.1294  0.9938]\n",
      "the estimated relative tf for iter 11 is [-0.0272  0.0314 -0.007  -0.0151 -0.0132 -0.1746  0.9942]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the estimated relative tf for iter 12 is [-0.0085  0.0182  0.0013 -0.0601  0.0433 -0.0883  0.9888]\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "#  MAIN \n",
    "##########\n",
    "\n",
    "# At the very first status \n",
    "is_viz = 1\n",
    "if is_viz:\n",
    "    pcd0.paint_uniform_color([1, 0, 1])\n",
    "    pcd1.paint_uniform_color([0, 0, 1])\n",
    "    o3d.visualization.draw_geometries([pcd0, pcd1], window_name=\"initial status\")\n",
    "\n",
    "# Initial condition \n",
    "def gen_noisy_but_reliable_inital():\n",
    "    rot_init = R.from_euler('xyz', true_rot_diff_rpy, degrees=True).as_rotvec() + 0.05*np.random.rand(3)\n",
    "    trans_init = true_trans_diff + np.random.rand(3)*(0.1*scale_up)\n",
    "    scale_init = 0.7* true_scale_diff\n",
    "    \n",
    "    eps = 0.0001 # +eps means: because zero initial should be avoided (see the symbolic equation of Je_rot!)\n",
    "    initial_state_vector = np.hstack((rot_init, trans_init, scale_init)) + eps \n",
    "    return initial_state_vector\n",
    "\n",
    "def identity_inital():\n",
    "    eps = 0.01\n",
    "    return np.array([eps, eps, eps, eps, eps, eps, 1.0])\n",
    "    # because after the update, the registered_src is expected to be equal to the target \n",
    "    # thus, the translation and rotataion would be zero and the relative scale must be 1.0\n",
    "\n",
    "init_guess = gen_noisy_but_reliable_inital() \n",
    "# init_guess = gen_noisy_but_reliable_inital() \n",
    "    # at the very first step, a moderate (i.e., not-identity) initial value is required \n",
    "    # because the cost function is highly nonlinear\n",
    "    # ps. try yourself using init_guess = identity_inital() rather than gen_noisy_but_reliable_inital()\n",
    "    #  the convergence speed would be deteriorated. (test yourself!)\n",
    "print(f\"init_guess is {init_guess}\")\n",
    "\n",
    "# NOTE\n",
    "# The number of correspondences and their spatial distirbution would affect the results\n",
    "# for example, \n",
    "# in the below example, \n",
    "# for the dragon dataset, it will converge (when we use the robust loss) even under 50% outliers while using skip=20\n",
    "# however, for the bunny dataset, which has the more smaller number of points, would not converge when we use skip=20 (skip=1 is then okay. try yourself!)\n",
    "    # ps. for the production level code, you also adaptively conclude when num_iters should be stopped in the icp_once (e.g., by tracking the df or residuals)\n",
    "        # by doing so, you need to prevent the solution from divergent.   \n",
    "# therefore, the what I want to say is for parameter tuning, we should well understand your dataset's characteristics (e.g., density, spatial distribution, etc.)\n",
    "\n",
    "# the robust loss parameter is also important\n",
    "# for dragon, we will use \n",
    "\"\"\"\n",
    "    alpha = 0\n",
    "    delta = 0.1\n",
    "    scalar_information = 10\n",
    "    epsilon = 1.0e-6\n",
    "    \n",
    "    , and pts_skip = 20\n",
    "\"\"\"\n",
    "\n",
    "# ICP starts  \n",
    "max_iter = 25\n",
    "src_pc, tgt_pc = [np.array(pc.points) for pc in [pcd0, pcd1]]\n",
    "for _iter in range(max_iter):\n",
    "    print(f\"\\n======================================\")\n",
    "    print(f\"  ==========   iter {_iter}   ==========\")\n",
    "    print(f\"======================================\")\n",
    "    # 1. optimize once \n",
    "    src_pc_before_updated = copy.deepcopy(src_pc)\n",
    "\n",
    "    outlier_ratio = 0.5 # test yourself up to 0.00 (no outlier) to 0.99\n",
    "    pts_skip = (10 + _iter) # for bunny (num points are small), use skip = 1 and for the dragon, use skip=20 is okay\n",
    "    num_iters = 20\n",
    "\n",
    "    if _iter < 0:\n",
    "        fix_scale=True \n",
    "    else:\n",
    "        fix_scale=False\n",
    "    tf_tangent, line_sets = icp_once(src_pc, tgt_pc, init_guess, skip=pts_skip, \\\n",
    "                                     outlier_ratio=outlier_ratio,\n",
    "                                     num_iters=num_iters,\n",
    "                                     fix_scale=fix_scale,\n",
    "                                     verbose=False) # if \"true\" correspondence is given (this is an tutorial for education purpose), using a few iteration okay ..\n",
    "\n",
    "    # 2. move the src to target and \n",
    "    est_rot3, est_trans3, est_scale = tf_tangent[:3], tf_tangent[3:6], tf_tangent[-1]\n",
    "    est_rotmat3x3 = rotmat.subs({rotvec: sf.V3(est_rot3)}).to_rotation_matrix().to_numpy()\n",
    "    src_pc_updated = est_scale*(est_rotmat3x3 @ src_pc.transpose()) + np.array([est_trans3]).transpose()\n",
    "    src_pc = src_pc_updated.transpose()\n",
    "    \n",
    "    init_guess = identity_inital()\n",
    "    # note: we explicitly update the source point cloud (i.e., registered), \n",
    "        # thus from the next step, we will use init_guess always equal to identity (toy example assumption)\n",
    "        # because, as already mentioned, after the update, the registered_src is expected to be equal to the target \n",
    "        # thus, the translation and rotataion would be zero and the relative scale must be 1.0\n",
    "        # in real world example, we need to use a domain knowledge to update the better initial (e.g., constant motion model, the prior knowledge of the object's metric scale, etc.)\n",
    "\n",
    "    # 3. re-correspondence\n",
    "    #  here, we can use the known true-correspondence because this is just a tutorial and affine transformation does not change the true correspondences \n",
    "    #   but in real world applications, kd-tree-like nearest neighbor search to find a newaly updated correspondences is required. \n",
    "\n",
    "    # 4. debug: Verify the result visually \n",
    "    if is_viz:\n",
    "        pcd0_Sim3_before_update = np2o3d(src_pc_before_updated)\n",
    "        pcd0_Sim3_before_update.paint_uniform_color([0.5, 0.5, 0.5])\n",
    "        \n",
    "        pcd0_Sim3_after_update = np2o3d(src_pc)\n",
    "        pcd0_Sim3_after_update.paint_uniform_color([25./255, 158./255, 243./255])\n",
    "        \n",
    "        pcd1.paint_uniform_color([0, 0, 1])\n",
    "        \n",
    "        line_sets[\"correct\"].paint_uniform_color([0, 0.737, 0.354])\n",
    "        if line_sets[\"false\"] is None:\n",
    "            line_sets[\"false\"] = copy.deepcopy(line_sets[\"correct\"])\n",
    "        else:\n",
    "            line_sets[\"false\"].paint_uniform_color([1.0, 0.0, 0.0])\n",
    "        \n",
    "        # draw before \n",
    "        o3d.visualization.draw_geometries([pcd0_Sim3_before_update, pcd1, \\\n",
    "                                           line_sets[\"correct\"], line_sets[\"false\"]], \\\n",
    "                                           window_name=f\"iteration {_iter} (gray: before update, blue: target)\")\n",
    "        \n",
    "        # draw after \n",
    "        o3d.visualization.draw_geometries([pcd0_Sim3_after_update, pcd1], \\\n",
    "                                           window_name=f\"iteration {_iter} (sky: after updated, blue: target)\")\n",
    "\n",
    "    # 5. if tf_tangent is smaller than a threshold, stop \n",
    "    print(\"\\n==========estimation==========\")\n",
    "    print(f\"delta rotation:\\n{est_rotmat3x3}\")\n",
    "    print(f\"delta translation:\\n{est_trans3}\")\n",
    "    print(f\"delta scale: {est_scale:.3f}\")\n",
    "    # TBA, e.g., if delta_translation < 0.01, break;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ Future works \n",
    "\n",
    "# 1: \n",
    "# Here, the false residual and its Hessian is directly incorporated within the normal equation \n",
    "# we can say this deweighting method implictily handles the outliers. \n",
    "# The next step you can do is to \"explictly remove\" the false correspondences from the pairs \n",
    "# e.g., using RANSAC \n",
    "\n",
    "# 2:\n",
    "# using parallel implmentation, boost the above per-point iterations faster \n",
    "\n",
    "# 3:\n",
    "# using not a point-to-point L2 distance, using point-to-plane (using normal and dot product) loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492bd669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
